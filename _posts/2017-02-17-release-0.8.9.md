---
layout: post
title:  "Release 0.8.9: Beta"
date:   2017-02-17
categories: page
---

Today, <a href="http://iao.hfuu.edu.cn/index.php/news/32-optimizationbenchmarking-beta">version `0.8.9`</a>, the "<a href="http://iao.hfuu.edu.cn/index.php/news/32-optimizationbenchmarking-beta">Patched Beta</a>" release of our {{ site.projectNameStyled }} software suite, was published. Compared to the <a href="{{ site.baseurl }}/page/2016/07/17/release-0.8.7.html">previous release</a>, this new version comes much closer to our goal to reduce the workload of researchers and practitioners during the development of algorithms to allow them to spend more time on thinking.

Besides all the functionality offered by the previous releases, it introduces a new process for obtaining high-level conclusions about problem hardness and algorithm behaviors. This process takes the raw data from experiments together with meta-information about algorithm setups and problem instances as input. It applies a sequence of machine learning technologies, namely curve fitting, clustering, and classification, to find which features make a problem instance hard and which algorithm setup parameters cause which algorithm behavior. We just submitted an article about this new process for review.

Like version <a href="{{ site.baseurl }}/page/2016/07/17/release-0.8.7.html">0.8.7</a>, our new software release comes in three flavors:

<ol>
<li><a href="https://github.com/optimizationBenchmarking/evaluator-gui/releases/download/0.8.9/evaluatorGui.jar">evaluator/GUI</a></li>
<li><a href="https://hub.docker.com/r/optimizationbenchmarking/evaluator-gui/">dockerized evaluator/GUI</a></li>
<li><a href="https://github.com/optimizationBenchmarking/evaluator-evaluator/releases/download/0.8.9/evaluator.jar">evaluator/CLI</a></li>
</ol>

The <a href="https://github.com/optimizationBenchmarking/evaluator-gui/releases/download/0.8.9/evaluatorGui.jar">GUI</a> is a stand-alone server providing a web-based interface for managing your experimental data, creating meta data, and running the evaluator.

The <a href="https://hub.docker.com/r/optimizationbenchmarking/evaluator-gui/">dockerized</a> version of the GUI comes as a, well, <a href="http://www.docker.com">Docker</a> image. This image has all the software requirements pre-installed, including the [`Java 8 OpenJDK`](http://openjdk.java.net/projects/jdk8/), [`TeX Live`](http://www.tug.org/texlive/), [`R`](https://www.r-project.org/), the needed `R` packages, and [`ghostscript`](http://ghostscript.com/). Using the evaluator as Docker image requires more disk space, but apart from Docker itself, you don't need to install anything. This seems to be quite a convenient scenario to me, given that you would otherwise need to install `R` and several of its packages, TeX Live, and such and such, to fully enjoy all features of our system.

The <a href="https://github.com/optimizationBenchmarking/evaluator-evaluator/releases/download/0.8.9/evaluator.jar">CLI</a>, i.e., the command line interface, contains only the pure evaluator component and is to be executed manually. This may make sense when running it as component of another, non-Java process. If you would want to use the software inside your Java program, you can just the libraries directly. If you want to know how to do that, <a href="mailto:{{ site.contactEmail }}>contact me</a>. 

Anyway, both the GUI and the CLI  come as stand-alone `jar`s and you can run them directly via `java -jar evaluatorGui.jar` (in case of the GUI) and `java -jar evaluator.jar` in case of the evaluator. In the dockerized GUI version, this is already automatically done for you.